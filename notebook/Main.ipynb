{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "from glob import glob\n",
    "from dask.diagnostics import ProgressBar\n",
    "import pickle\n",
    "import re\n",
    "import konlpy\n",
    "# logging\n",
    "import logging\n",
    "logging.basicConfig(filename='exo.log',level=logging.INFO)\n",
    "# Display progress\n",
    "pbar = ProgressBar()\n",
    "pbar.register()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load sentences to processing unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fnames = glob('../data/sents/*.txt')\n",
    "# fnames = fnames[:3]\n",
    "b = db.from_filenames(fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_terms = None\n",
    "doosan_terms = None\n",
    "with open('../data/dict/wiki_terms.pickle', 'rb') as fin:\n",
    "    wiki_terms = pickle.load(fin)\n",
    "with open('../data/dict/doosan_terms.pickle', 'rb') as fin:\n",
    "    doosan_terms = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def per_partition(lines):\n",
    "    \"\"\"\n",
    "    위에서 사용한 `from_filenames`는 텍스트 파일을 문장 집합 형태로 로드한 후, \n",
    "    규모에 따라 수십개의 파티션으로 나눔\n",
    "    예) [문장1, 문장2, 문장3, ...], [문장4, 문장5, 문장6, ...], ...\n",
    "    \n",
    "    파티션은 컴퓨터에서 활용 가능한 각 코어로 분배됨.\n",
    "    \n",
    "    이 함수에서는\n",
    "    1) 파티션에 소속된 각 파일에 대해 원하는 처리를 수행\n",
    "    2) 결과물을 반환\n",
    "    \"\"\"\n",
    "    # Load the dictionary terms    \n",
    "    wiki_terms = None\n",
    "    doosan_terms = None\n",
    "    with open('../data/dict/wiki_terms.pickle', 'rb') as fin:\n",
    "        wiki_terms = pickle.load(fin)\n",
    "    with open('../data/dict/doosan_terms.pickle', 'rb') as fin:\n",
    "        doosan_terms = pickle.load(fin)\n",
    "        \n",
    "    # Initialize the necessary components \n",
    "    # before iterating over the sentences\n",
    "    pattern1 = re.compile('(\\\"|\\'|“|”|‘|’|《|》|\\.|,|•|-)')\n",
    "    pattern2 = re.compile('(·|-|\\.)')\n",
    "    tagger_twitter = konlpy.tag.Twitter()\n",
    "    except_list = [\"단계\", \"대해\", \"다소\", \"두\", \"세\", \"네\", \"오늘날\", \"하나하나\", \"정도\", \"처음\",\"경\", \"자체\", \"후\",\"하나\", \"앞\", \"뒤\", \"위\",\"아래\", \"수\",\"데\", \"내\", \"음\", \"번\", \"그후\", \"이하\",\"이상\", \"여기\", \"권\", \"당시\", \"책\", \"년\", \"달\", \"월\", \"해\", \"일\", \"말\", \"초\", \"째\", \"사실상\", \"자신\", \"역시\", \"각종\", \"의\", \"앞\", \"지\",\"속\", \"대\", \"전\", \"주요\", \"일\", \"제\", \"해도\", \"처\", \"이\", \"저\", \"수\", \"그\", \"때\", \"가지\", \"이후\", \"매우\", \"등\", \"못\", \"스스로\", \"오직\", \"이기\", \"볼\", \"초\", \"약\", \"중\", \"상\", \"개\", \"주\", \"예\", \"이전\", \"이번\", \"채\", \"안\", \"경우\"]\n",
    "        \n",
    "    # Process each sentence\n",
    "    # enumerate(): iterable의 인덱스를 함께 반환하는 helper 함수\n",
    "    for i, line in enumerate(lines):\n",
    "        try:\n",
    "            sentence = line.split('\\t')[3]\n",
    "            original_sentence = sentence[:]\n",
    "        except ValueError as e:\n",
    "            logging.warning(\"error parsing %s\" % sentence)\n",
    "            continue\n",
    "            \n",
    "        # remove special characters (결과값이 예전과 약간 다를 수 있으니 검증 필요)\n",
    "        ## 아예 제거\n",
    "        sentence = pattern1.sub('', sentence)\n",
    "        ## 공백으로 치환\n",
    "        sentence = pattern2.sub(' ', sentence)\n",
    "\n",
    "        # Perform tagging with twitter\n",
    "        word_tag_list = tagger_twitter.pos(sentence)\n",
    "        # (use list comprehension whenever possible)\n",
    "        nouns = [word for word, pos in word_tag_list if pos in ('Alpha', 'Number', 'Noun')]\n",
    "        \n",
    "        # 결과값 반환\n",
    "        return nouns\n",
    "        \n",
    "def aggregate(list_of_nouns):\n",
    "    for noun_list in list_of_nouns:\n",
    "        print(noun_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b2 = b.reduction(per_partition, aggregate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed |  3.2s['지미', '카터', '민주당', '출신', '미국', '39', '대통령']\n",
      "['1792', '년', '영국', '시인', '퍼시', '비시', '셸리']\n",
      "['2013', '년', '2014', '년', 'MBC', '우리', '일밤', '진짜', '사나이']\n",
      "[########################################] | 100% Completed |  3.3s\n"
     ]
    }
   ],
   "source": [
    "b2.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "konlpy",
   "language": "python",
   "name": "konlpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
